{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Igwe-Ugo/Facial_recognition_system/blob/main/face_recog_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWWOUv7lRdal"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbo3m7GJYwX5"
      },
      "outputs": [],
      "source": [
        "# 1. Use TensorFlow's mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I51lDHkZRlcF"
      },
      "outputs": [],
      "source": [
        "# Define the dataset paths\n",
        "data_path = 'drive/MyDrive/face_data'\n",
        "train_path = os.path.join(data_path, 'train')\n",
        "val_path = os.path.join(data_path, 'val')\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(train_path, exist_ok=True)\n",
        "os.makedirs(val_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbQI3MywR0WD"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    return tf.cast(img, tf.float16)  # Cast to float16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Ao96lFS5oD"
      },
      "outputs": [],
      "source": [
        "def create_triplet_dataset(directory):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for person_dir in os.listdir(directory):\n",
        "        person_path = os.path.join(directory, person_dir)\n",
        "        if os.path.isdir(person_path):\n",
        "            for image_name in os.listdir(person_path):\n",
        "                image_paths.append(os.path.join(person_path, image_name))\n",
        "                labels.append(person_dir)\n",
        "\n",
        "    def generate_triplets():\n",
        "        while True:\n",
        "            anchor_label = random.choice(labels)\n",
        "            anchor_image = random.choice([img for img, lbl in zip(image_paths, labels) if lbl == anchor_label])\n",
        "            positive_image = random.choice([img for img, lbl in zip(image_paths, labels) if lbl == anchor_label and img != anchor_image])\n",
        "            negative_label = random.choice([lbl for lbl in labels if lbl != anchor_label])\n",
        "            negative_image = random.choice([img for img, lbl in zip(image_paths, labels) if lbl == negative_label])\n",
        "\n",
        "            yield (load_and_preprocess_image(anchor_image),\n",
        "                   load_and_preprocess_image(positive_image),\n",
        "                   load_and_preprocess_image(negative_image))\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generate_triplets,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4gDfl01VpJG"
      },
      "outputs": [],
      "source": [
        "def create_base_network(input_shape):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    return Model(inputs=base_model.input, outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAYFiI92VrqC"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    anchor, positive, negative = y_pred[:,:128], y_pred[:,128:256], y_pred[:,256:]\n",
        "\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "\n",
        "    basic_loss = pos_dist - neg_dist + alpha\n",
        "    loss = tf.maximum(basic_loss, 0.0)\n",
        "    return tf.reduce_mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn0bkOspVt4q"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape):\n",
        "    base_network = create_base_network(input_shape)\n",
        "\n",
        "    input_anchor = Input(shape=input_shape, name='input_1')\n",
        "    input_positive = Input(shape=input_shape, name='input_2')\n",
        "    input_negative = Input(shape=input_shape, name='input_3')\n",
        "\n",
        "    embedding_anchor = base_network(input_anchor)\n",
        "    embedding_positive = base_network(input_positive)\n",
        "    embedding_negative = base_network(input_negative)\n",
        "\n",
        "    output = Lambda(lambda x: tf.concat(x, axis=-1))([embedding_anchor, embedding_positive, embedding_negative])\n",
        "\n",
        "    model = Model(inputs=[input_anchor, input_positive, input_negative], outputs=output)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.0001)\n",
        "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
        "\n",
        "    model.compile(loss=triplet_loss, optimizer=optimizer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8MDwzX4V7sP"
      },
      "outputs": [],
      "source": [
        "# Modify the prepare_dataset function to handle unpacked arguments\n",
        "def prepare_dataset(anchor, positive, negative):\n",
        "    inputs = {'input_1': anchor, 'input_2': positive, 'input_3': negative}\n",
        "    labels = tf.zeros_like(anchor)[:, 0, 0]  # Dummy labels\n",
        "    return inputs, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqyw06JDVv17"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = create_triplet_dataset(train_path)\n",
        "val_dataset = create_triplet_dataset(val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "areYBX9bV2kR"
      },
      "outputs": [],
      "source": [
        "# Prepare the datasets\n",
        "batch_size = 16  # 2. Reduced batch size\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAmv5jDiV5ZF"
      },
      "outputs": [],
      "source": [
        "# Create and compile the model\n",
        "input_shape = (224, 224, 3)\n",
        "model = create_model(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_8VExfzeAf6"
      },
      "outputs": [],
      "source": [
        "# Define optimizer globally\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkTdNClyWA71"
      },
      "outputs": [],
      "source": [
        "train_dataset = (train_dataset\n",
        "                 .cache()\n",
        "                 .shuffle(1000)\n",
        "                 .batch(batch_size)\n",
        "                 .map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                 .prefetch(tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPh3OK5pe7Jr"
      },
      "outputs": [],
      "source": [
        "val_dataset = (val_dataset\n",
        "               .cache()\n",
        "               .batch(batch_size)\n",
        "               .map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "               .prefetch(tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k4VdzshZZ51"
      },
      "outputs": [],
      "source": [
        "# 4. Use a custom training loop for more control\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = triplet_loss(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq6z0owecKMU"
      },
      "outputs": [],
      "source": [
        "# Check dataset cardinality\n",
        "train_cardinality = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "val_cardinality = tf.data.experimental.cardinality(val_dataset).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzn88qeUWI64",
        "outputId": "65eeceab-065b-4fd6-dca7-6d65571ca5ec"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "Step 0, Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = tf.keras.metrics.Mean()\n",
        "    for step, (inputs, labels) in enumerate(train_dataset):\n",
        "        loss = train_step(inputs, labels)\n",
        "        train_loss.update_state(loss)\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Step {step}, Loss: {train_loss.result():.4f}\")\n",
        "\n",
        "        # Clear memory periodically\n",
        "        if step % 100 == 0:\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "        # Break if dataset is infinite (optional)\n",
        "        if train_cardinality < 0 and step >= 1000:  # Adjust this number as needed\n",
        "            break\n",
        "\n",
        "    print(f\"Training Loss: {train_loss.result():.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    val_loss = tf.keras.metrics.Mean()\n",
        "    for inputs, labels in val_dataset:\n",
        "        val_predictions = model(inputs, training=False)\n",
        "        val_batch_loss = triplet_loss(labels, val_predictions)\n",
        "        val_loss.update_state(val_batch_loss)\n",
        "\n",
        "        # Break if dataset is infinite (optional)\n",
        "        if val_cardinality < 0 and val_loss.count.numpy() >= 100:  # Adjust this number as needed\n",
        "            break\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss.result():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr0KEckwWLHo"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save('face_recognition_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocyLye3qWIMs"
      },
      "outputs": [],
      "source": [
        "# Save the base network for generating embeddings\n",
        "base_network = create_base_network(input_shape)\n",
        "base_network.set_weights(model.get_layer(base_network.name).get_weights())\n",
        "base_network.save('face_embedding_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4741L3vDWDuQ"
      },
      "outputs": [],
      "source": [
        "print(\"Training completed. Models saved.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Bvh04zydbgLHHFHn2o9Gjt-zWJ2COiwP",
      "authorship_tag": "ABX9TyNXoZtedZuf6lyPMft7rVJX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}